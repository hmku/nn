{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a95fe618-6b6c-4823-82d8-b18a2bc8414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de54e26d-119c-4102-ad7b-2f0e381ae087",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n = 200_000\n",
    "data_X = np.random.normal(0, 1, size=(3, n))\n",
    "x1, x2, x3 = data_X\n",
    "data_y = 0.5 * x1 + 0.2 * x2 + 0.3 * x3 + np.random.normal(0, 0.5, size=n)\n",
    "\n",
    "train_X, test_X = data_X[:,:3*n//4], data_X[:,3*n//4:]\n",
    "train_y, test_y = data_y[:3*n//4], data_y[3*n//4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5840f43a-c916-496d-87d6-c88629e42c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker:\n",
    "    def __init__(self, size):\n",
    "        self.loss_buffer = [np.nan for _ in range(size)]\n",
    "        self.var_y_buffer = [np.nan for _ in range(size)]\n",
    "        self.i = 0\n",
    "        self.size = size\n",
    "\n",
    "    def update(self, loss, var_y):\n",
    "        self.loss_buffer[self.i] = loss\n",
    "        self.var_y_buffer[self.i] = var_y\n",
    "        self.i = (self.i + 1) % self.size\n",
    "\n",
    "    def summary(self):\n",
    "        return f\"loss: {np.nanmean(self.loss_buffer)}, r2: {1 - np.nanmean(self.loss_buffer) / np.nanmean(self.var_y_buffer)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad0c35e5-6e40-40a7-a1b8-96dc51199cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.6889609417519895, r2: -1.8467685390179258 , val loss: 0.9768736640470798, val r2: -0.5528011953304315\n",
      "loss: 0.31879432623994963, r2: 0.4960629322566791 , val loss: 0.26286516086407496, val r2: 0.5821595452687013\n",
      "loss: 0.2921728750800388, r2: 0.5346411097751934 , val loss: 0.2612449967291477, val r2: 0.5847348965120994\n",
      "loss: 0.28036644584378007, r2: 0.5537739855076422 , val loss: 0.2593113766197729, val r2: 0.5878085054419513\n",
      "loss: 0.274478370629519, r2: 0.5632914433364371 , val loss: 0.2631309908157414, val r2: 0.5817369920953542\n",
      "loss: 0.2714727447275986, r2: 0.5676584041538754 , val loss: 0.2527701086790954, val r2: 0.5982062559915772\n",
      "loss: 0.2693575165466226, r2: 0.5719487600701045 , val loss: 0.25511114773803767, val r2: 0.5944850294063699\n",
      "loss: 0.26699687092998103, r2: 0.5755391180933324 , val loss: 0.2563190096891101, val r2: 0.5925650580216884\n",
      "loss: 0.2660645029390302, r2: 0.5784620400168048 , val loss: 0.25244711530640174, val r2: 0.5987196739632775\n",
      "loss: 0.26432691048936097, r2: 0.5806736621827489 , val loss: 0.2533579421105533, val r2: 0.5972718583426105\n",
      "loss: 0.26169898570484207, r2: 0.5846973867463743 , val loss: 0.2514201012126433, val r2: 0.6003521764772694\n",
      "loss: 0.25683866279242273, r2: 0.592485556287319 , val loss: 0.2563705745346723, val r2: 0.5924830925058056\n"
     ]
    }
   ],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.has_params = False\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        z = np.maximum(x, 0)\n",
    "        return z\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # d_loss / d_za * d_za / d_z\n",
    "        grad_out = np.einsum(\"bi,bi->bi\", grad, self.x > 0)\n",
    "        self.x = None\n",
    "        return grad_out\n",
    "\n",
    "    def step_fn(self, lr):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, input_dim, output_dim, seed=0):\n",
    "        np.random.seed(seed)\n",
    "        self.has_params = True\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weights = np.random.normal(\n",
    "            0, np.sqrt(2 / self.input_dim), size=(self.output_dim, self.input_dim)\n",
    "        )\n",
    "        self.bias = np.zeros((self.output_dim, 1))\n",
    "        self.x = None\n",
    "        self.grad_cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(self.input_dim, self.output_dim, x)\n",
    "        B, x_dim = x.shape\n",
    "        # print(\"forward\", B, x_dim)\n",
    "        assert x_dim == self.input_dim, (x_dim, self.input_dim)\n",
    "        self.x = x\n",
    "        # (out, in) * (B, in) -> (B, out)\n",
    "        z = np.einsum(\"ji,bi->bj\", self.weights, self.x) + self.bias.T\n",
    "        # print(\"forward\", x.shape, self.weights.shape, self.bias.shape, z.shape)\n",
    "        return z\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # d_loss / d_z * d_z / d_w, d_loss / d_z * d_z / d_bias\n",
    "        # print(\"backward\", grad.shape, self.x.shape)\n",
    "        self.grad_cache = {\n",
    "            \"weights\": np.einsum(\"bj,bi->ji\", grad, self.x),\n",
    "            \"bias\": np.einsum(\"bj->j\", grad).reshape(-1, 1),\n",
    "        }\n",
    "\n",
    "        # d_loss / d_z * d_z / d_x\n",
    "        # print(grad.shape, self.weights.T.shape)\n",
    "        grad_out = np.einsum(\"bj,ji->bi\", grad, self.weights)\n",
    "        self.x = None\n",
    "        # print(\"backward1\", grad_out.shape)\n",
    "        return grad_out\n",
    "\n",
    "    def step_fn(self, lr):\n",
    "        self.weights -= lr * self.grad_cache[\"weights\"]\n",
    "        # print(\"step_fn\", self.bias.shape, self.grad_cache[\"bias\"].shape)\n",
    "        self.bias -= lr * self.grad_cache[\"bias\"]\n",
    "        self.grad_cache = None\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layers = [\n",
    "            Linear(input_dim, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, output_dim),\n",
    "        ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        for layer in self.layers[::-1]:\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "    def step_fn(self, lr):\n",
    "        for layer in self.layers:\n",
    "            if layer.has_params:\n",
    "                layer.step_fn(lr)\n",
    "\n",
    "\n",
    "def loss_fn(x, y):\n",
    "    return np.mean((y - x) ** 2), -2 * (y - x) / x.shape[0]\n",
    "\n",
    "\n",
    "def train():\n",
    "    tracker = Tracker(size=1000)\n",
    "    model = MLP(3, 64, 1)\n",
    "\n",
    "    batch_size = 128\n",
    "    for i, start in enumerate(range(0, train_X.shape[1], batch_size)):\n",
    "        end = start + batch_size\n",
    "        x = train_X[:, start:end].T\n",
    "        y = train_y[start:end].reshape(-1, 1)\n",
    "\n",
    "        # for i, (x, y) in enumerate(zip(train_X.T, train_y)):\n",
    "        # print(x.shape, y.shape)\n",
    "        z = model.forward(x)\n",
    "        loss, grad = loss_fn(z, y)\n",
    "        model.backward(grad)\n",
    "        model.step_fn(lr=0.01)\n",
    "\n",
    "        tracker.update(loss=loss, var_y=np.mean(y**2))\n",
    "        if i % 100 == 0:\n",
    "            test_z = model.forward(test_X.T)\n",
    "            val_loss, _ = loss_fn(test_z, test_y.reshape(-1, 1))\n",
    "            print(\n",
    "                tracker.summary(),\n",
    "                f\", val loss: {val_loss}, val r2: {1 - val_loss / np.mean(test_y**2)}\",\n",
    "            )\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c70935f-6124-4369-8c9b-ceaf59349781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression weights: [0.49957827 0.20149033 0.30107705]\n",
      "MSE: 0.2484, R^2: 0.6051\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Closed-form solution w = (X X^T)^-1 X y^T\n",
    "# train_X: (features, samples), train_y: (samples,)\n",
    "XtX = train_X @ train_X.T          # (3,3)\n",
    "Xty = train_X @ train_y            # (3,)\n",
    "weights = np.linalg.solve(XtX, Xty)  # (3,)\n",
    "\n",
    "print(\"Linear regression weights:\", weights)\n",
    "\n",
    "# Predictions\n",
    "preds = weights @ test_X  # shape: (10000,)\n",
    "# weights: (3,), train_X: (3,10000) -> (10000,)\n",
    "\n",
    "# Compute loss and R^2\n",
    "mse = np.mean((test_y - preds)**2)\n",
    "r2 = 1 - np.sum((test_y - preds)**2) / np.sum((test_y - np.mean(test_y))**2)\n",
    "\n",
    "print(f\"MSE: {mse:.4f}, R^2: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
