{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "551380cd-8463-4391-a206-525c7524bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ada431-21ac-4191-b2c4-cecbcac7e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9348e43c-55e9-4b92-8a3d-7a9975d16c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ce: -\\sum_i P(x_i) * log(Q(x_i))\n",
    "# y = outcome, p = prediction\n",
    "# bce: (1 - y) * Log(1-p) + y * log(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6091284-89e6-4bc6-bc07-aef82bff9549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid\n",
    "# logits q0, q1\n",
    "# e^q0 / (e^q0 + e^q1) \n",
    "# 1 / (1 + e^(-q1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b93851b-de5c-4d74-9654-ea6a74ea7366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44589139 -0.14182575  0.42520219  0.26549794  0.01300365] 0.10740014109110481\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "n = 100_000\n",
    "n_X = 5\n",
    "X = np.random.normal(0, 1, size=(n, n_X))\n",
    "betas = np.random.normal(0, 0.3, size=n_X)\n",
    "intercept = np.random.normal(0, 0.3)\n",
    "eps = np.random.normal(0, 0.3, size=n)\n",
    "print(betas, intercept)\n",
    "\n",
    "z = X @ betas + intercept + eps\n",
    "y = (1 / (1 + np.exp(-z)) > 0.5).astype(int)\n",
    "\n",
    "train_pct = 0.9\n",
    "train_n = int(train_pct * n)\n",
    "train_X, train_y = X[:train_n], y[:train_n]\n",
    "test_X, test_y = X[train_n:], y[train_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "088b623f-84d6-401c-b2f9-cad3fac9e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(p, y):\n",
    "    loss = -np.mean((1 - y) * np.log(1 - p) + y * np.log(p))\n",
    "    grad = ((1-y)/(1-p) - y/p) / len(y)\n",
    "    return loss, grad\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x, *, no_grad):\n",
    "        if not no_grad:\n",
    "            self.x = x\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # d_loss / d_z * d_z / d_x\n",
    "        # f(x) = 1 / (1 + e^-x)\n",
    "        # d/dx f(x) = -1 / (1 + e^-x)^2 * (-e^-x)\n",
    "        # = e^-x / (1 + e^-x)^2\n",
    "        dz_dx = (np.exp(-self.x) / (1 + np.exp(-self.x)) ** 2)\n",
    "        self.x = None\n",
    "        return np.einsum(\"bj,bj->bj\", grad, dz_dx)\n",
    "\n",
    "    def step_fn(self, *, lr):\n",
    "        pass\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, *, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.W = np.random.normal(\n",
    "            0, np.sqrt(2 / self.input_dim), size=(output_dim, input_dim)\n",
    "        )\n",
    "        self.b = np.zeros(output_dim)\n",
    "        self.x = None\n",
    "        self.grad_cache = {}\n",
    "\n",
    "    def forward(self, x, *, no_grad):\n",
    "        if not no_grad:\n",
    "            self.x = x\n",
    "        wx = np.einsum(\"ji,bi->bj\", self.W, x)\n",
    "        return wx + self.b\n",
    "\n",
    "    def backward(self, grad):\n",
    "        b, j = grad.shape\n",
    "        # return d_loss / d_x = d_loss / d_z * d_z / d_x\n",
    "        # store d_loss / d_W = d_loss / d_z * d_z / d_W\n",
    "        # store d_loss / d_b = d_loss / d_z * d_z / d_b\n",
    "\n",
    "        # w1 * x1 + w2 * x2 + b\n",
    "        dz_dx = self.W  # ji\n",
    "        dz_dW = self.x  # Bi\n",
    "        dz_db = 1\n",
    "        self.grad_cache[\"W\"] = np.einsum(\"bj,bi->ji\", grad, dz_dW)\n",
    "        self.grad_cache[\"b\"] = np.einsum(\"bj,->j\", grad, dz_db)\n",
    "        self.x = None\n",
    "        return np.einsum(\"bj,ji->bi\", grad, dz_dx)\n",
    "\n",
    "    def step_fn(self, *, lr):\n",
    "        self.W -= lr * self.grad_cache[\"W\"]\n",
    "        self.b -= lr * self.grad_cache[\"b\"]\n",
    "        self.grad_cache = {}\n",
    "\n",
    "\n",
    "class LogReg:\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.linear = Linear(input_dim=input_dim, output_dim=1)\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self, x, no_grad=False):\n",
    "        x = self.linear.forward(x, no_grad=no_grad)\n",
    "        x = self.sigmoid.forward(x, no_grad=no_grad)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        grad = self.sigmoid.backward(grad)\n",
    "        grad = self.linear.backward(grad)\n",
    "        return grad\n",
    "\n",
    "    def step_fn(self, *, lr):\n",
    "        self.sigmoid.step_fn(lr=lr) # doesn't do anything\n",
    "        self.linear.step_fn(lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ccf652a4-283f-49d2-b0c6-a8a04b97115e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0426366186325917 0.1733359007417484 0.9970746370598077 0.5035\n",
      "25 0.6361983949684096 0.1733359007417484 0.6171528049929246 0.6688\n",
      "50 0.48987423878073494 0.1733359007417484 0.45532785529878755 0.7915\n",
      "75 0.38770475558579814 0.1733359007417484 0.3852365364492618 0.8466\n",
      "100 0.3479609680802494 0.1733359007417484 0.3524435404459022 0.8667\n",
      "125 0.37148281850400594 0.1733359007417484 0.3334678304468279 0.8726\n",
      "150 0.3226197873797627 0.1733359007417484 0.3221778312128625 0.874\n",
      "175 0.33306451644294643 0.1733359007417484 0.3139797520219594 0.8748\n",
      "200 0.3253706665103777 0.1733359007417484 0.30776854176652924 0.8755\n",
      "225 0.3473251564289802 0.1733359007417484 0.3029058829616264 0.8759\n",
      "250 0.32217171724555177 0.1733359007417484 0.2991764472546533 0.8753\n",
      "275 0.293676455364145 0.1733359007417484 0.2959045277523712 0.8752\n",
      "300 0.31097113579570873 0.1733359007417484 0.29314526563889515 0.8753\n",
      "325 0.31719618971314134 0.1733359007417484 0.2908719913705171 0.8751\n",
      "350 0.3196167599504045 0.1733359007417484 0.28886023792283916 0.8761\n"
     ]
    }
   ],
   "source": [
    "model = LogReg(n_X)\n",
    "lr = 0.1\n",
    "batch_size = 256\n",
    "\n",
    "for it, idx in enumerate(range(0, train_n, batch_size)):\n",
    "    batch_X = train_X[idx:idx+batch_size]\n",
    "    batch_y = train_y[idx:idx+batch_size].reshape(-1, 1)\n",
    "    \n",
    "    p = model.forward(batch_X)\n",
    "    loss, grad = bce_loss(p, batch_y)\n",
    "    _ = model.backward(grad)\n",
    "    model.step_fn(lr=lr)\n",
    "    if it % 25 == 0:\n",
    "        test_y_ = test_y.reshape(-1, 1)\n",
    "        val_p = model.forward(test_X)\n",
    "        val_loss, _ = bce_loss(val_p, test_y_)\n",
    "        val_accuracy = ((val_p > 0.5).astype(int) == test_y_).mean()\n",
    "        print(it, loss, train_loss, val_loss, val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "71e4830e-e406-45f7-8f72-a6d3da468fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5591333333333334)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "031aa31a-cfff-4eea-a446-0c08588d14ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5546)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e0ad060e-d47d-4212-b199-c129403aaa95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.44589139, -0.14182575,  0.42520219,  0.26549794,  0.01300365]),\n",
       " 0.10740014109110481)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e7a7e2f7-1199-4130-bc17-71397578d0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.85469171, -0.55072033,  1.77522004,  1.10502778,  0.05569297]]),\n",
       " array([0.44974052]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.W, model.linear.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ffeef5-f343-4e3a-87e5-a1fe5d06d3b8",
   "metadata": {},
   "source": [
    "# baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7df39caa-20bc-40bb-8d67-6245aea37411",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a0c959f-20c3-4b10-b80d-1bc661bc0ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8721111111111111"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b66bd3a1-66f5-4061-a1f5-ee7d2466b529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8757"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7655256f-6520-4b57-b2af-310411ca89b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2715855723687466\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "y_prob = clf.predict_proba(test_X)[:, 1]\n",
    "loss = log_loss(test_y, y_prob)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0196d2d7-851e-4ec3-9f8c-f0cd7bc270c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2.65052164, -0.8390301 ,  2.5420956 ,  1.58524263,  0.09128322]]),\n",
       " array([0.65365801]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d25bd-3287-444b-b0e6-760bab6979d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83080bf1-e066-4ea6-b6df-42848b4fd12c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (conda)",
   "language": "python",
   "name": "py313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
