{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8cbcfd-c0ac-438b-b79d-11b1a45a3850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.20.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/hku/miniconda3/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/hku/miniconda3/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Using cached fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch)\n",
      "  Downloading triton-3.5.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading markupsafe-3.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
      "Downloading torch-2.9.1-cp313-cp313-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m  \u001b[33m0:12:35\u001b[0mm0:00:01\u001b[0m00:19\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m  \u001b[33m0:09:48\u001b[0mm0:00:01\u001b[0m00:15\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:03:58\u001b[0mm0:00:01\u001b[0m00:06\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m  \u001b[33m0:04:29\u001b[0mm0:00:01\u001b[0m00:08\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m  \u001b[33m0:01:50\u001b[0mm0:00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.5.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:03:14\u001b[0mm0:00:01\u001b[0m00:04\u001b[0mm\n",
      "\u001b[?25hDownloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading filelock-3.20.2-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/24\u001b[0m [torch]m23/24\u001b[0m [torch]-cusolver-cu12]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 filelock-3.20.2 fsspec-2025.12.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.6.1 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 sympy-1.14.0 torch-2.9.1 triton-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52070983-131f-4755-be51-e1f94d414808",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention with optional KV caching.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor,\n",
    "        kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        use_cache: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor [batch_size, seq_len, d_model]\n",
    "            kv_cache: Optional tuple of (cached_k, cached_v) from previous steps\n",
    "            use_cache: Whether to return updated cache\n",
    "            \n",
    "        Returns:\n",
    "            output: Attention output [batch_size, seq_len, d_model]\n",
    "            new_cache: Updated (k, v) cache if use_cache=True, else None\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # TODO: Implement query, key, value projections\n",
    "        # Hint: Use self.W_q, self.W_k, self.W_v\n",
    "        q = self.W_q(x)  # [batch_size, seq_len, d_model]\n",
    "        k = self.W_k(x)  # [batch_size, seq_len, d_model]\n",
    "        v = self.W_v(x)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # TODO: Reshape for multi-head attention\n",
    "        # Hint: Split d_model into n_heads and d_k\n",
    "        # Target shape: [batch_size, n_heads, seq_len, d_k]\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # TODO: Implement KV cache logic\n",
    "        # If kv_cache is provided, concatenate cached k,v with new k,v\n",
    "        # This allows us to reuse computations from previous tokens\n",
    "        if kv_cache is not None:\n",
    "            cached_k, cached_v = kv_cache\n",
    "            # IMPLEMENT: Concatenate along sequence dimension\n",
    "            k = torch.cat([cached_k, k], dim=2)\n",
    "            v = torch.cat([cached_v, v], dim=2)\n",
    "        \n",
    "        # TODO: Compute scaled dot-product attention\n",
    "        # Hint: scores = (Q @ K^T) / sqrt(d_k), then softmax, then @ V\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # TODO: Reshape back and apply output projection\n",
    "        # Hint: Reverse the multi-head split\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        # TODO: Prepare cache for return\n",
    "        # If use_cache=True, return the full k,v tensors for next iteration\n",
    "        new_cache = (k, v) if use_cache else None\n",
    "        \n",
    "        return output, new_cache\n",
    "\n",
    "\n",
    "class SimpleTransformerBlock(nn.Module):\n",
    "    \"\"\"Single transformer block with KV cache support.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        use_cache: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:\n",
    "        # Self-attention with residual\n",
    "        attn_out, new_cache = self.attn(self.ln1(x), kv_cache, use_cache)\n",
    "        x = x + attn_out\n",
    "        \n",
    "        # Feedforward with residual\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        \n",
    "        return x, new_cache\n",
    "\n",
    "\n",
    "# ============= TEST AND DEMO CODE =============\n",
    "\n",
    "def test_kv_cache():\n",
    "    \"\"\"Test that KV cache produces identical results to full attention.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Testing KV Cache Implementation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Setup\n",
    "    batch_size = 2\n",
    "    d_model = 64\n",
    "    n_heads = 4\n",
    "    seq_len = 10\n",
    "    \n",
    "    model = SimpleTransformerBlock(d_model, n_heads, d_ff=128)\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate a full sequence\n",
    "    full_sequence = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Method 1: Process full sequence at once (standard)\n",
    "    with torch.no_grad():\n",
    "        output_full, _ = model(full_sequence, use_cache=False)\n",
    "    \n",
    "    # Method 2: Process token-by-token with KV cache (autoregressive)\n",
    "    outputs_cached = []\n",
    "    cache = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(seq_len):\n",
    "            token = full_sequence[:, i:i+1, :]  # [batch_size, 1, d_model]\n",
    "            output, cache = model(token, kv_cache=cache, use_cache=True)\n",
    "            outputs_cached.append(output)\n",
    "    \n",
    "    output_cached = torch.cat(outputs_cached, dim=1)\n",
    "    \n",
    "    # Compare results\n",
    "    max_diff = (output_full - output_cached).abs().max().item()\n",
    "    print(f\"\\n✓ Full sequence shape: {output_full.shape}\")\n",
    "    print(f\"✓ Cached sequence shape: {output_cached.shape}\")\n",
    "    print(f\"\\nMaximum difference: {max_diff:.2e}\")\n",
    "    \n",
    "    if max_diff < 1e-5:\n",
    "        print(\"✓ SUCCESS! KV cache produces identical results.\")\n",
    "    else:\n",
    "        print(\"✗ WARNING: Results differ. Check your implementation.\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Performance Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        model(full_sequence[:, :1, :])\n",
    "    \n",
    "    # Time full attention\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(100):\n",
    "            model(full_sequence)\n",
    "    full_time = time.time() - start\n",
    "    \n",
    "    # Time cached attention\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(100):\n",
    "            cache = None\n",
    "            for i in range(seq_len):\n",
    "                _, cache = model(full_sequence[:, i:i+1, :], cache, use_cache=True)\n",
    "    cached_time = time.time() - start\n",
    "    \n",
    "    print(f\"\\nFull attention (100 runs): {full_time:.3f}s\")\n",
    "    print(f\"Cached attention (100 runs): {cached_time:.3f}s\")\n",
    "    print(f\"Speedup: {full_time/cached_time:.2f}x\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Memory Usage (Cached K/V)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if cache is not None:\n",
    "        cached_k, cached_v = cache\n",
    "        print(f\"\\nCached K shape: {cached_k.shape}\")\n",
    "        print(f\"Cached V shape: {cached_v.shape}\")\n",
    "        total_elements = cached_k.numel() + cached_v.numel()\n",
    "        memory_mb = total_elements * 4 / (1024**2)  # Assume float32\n",
    "        print(f\"Total cache memory: ~{memory_mb:.2f} MB\")\n",
    "\n",
    "\n",
    "def demonstrate_autoregressive_generation():\n",
    "    \"\"\"Simulate autoregressive generation with KV cache.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Simulating Autoregressive Generation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    d_model = 64\n",
    "    model = SimpleTransformerBlock(d_model, n_heads=4, d_ff=128)\n",
    "    model.eval()\n",
    "    \n",
    "    # Start with a \"prompt\" (e.g., 3 tokens)\n",
    "    prompt = torch.randn(1, 3, d_model)\n",
    "    \n",
    "    print(\"\\n1. Processing prompt (3 tokens)...\")\n",
    "    with torch.no_grad():\n",
    "        _, cache = model(prompt, use_cache=True)\n",
    "    \n",
    "    print(f\"   Cache initialized. K/V shape: {cache[0].shape}\")\n",
    "    \n",
    "    # Generate 5 new tokens autoregressively\n",
    "    print(\"\\n2. Generating tokens one by one...\")\n",
    "    for i in range(5):\n",
    "        new_token = torch.randn(1, 1, d_model)\n",
    "        with torch.no_grad():\n",
    "            output, cache = model(new_token, kv_cache=cache, use_cache=True)\n",
    "        print(f\"   Token {i+1}: Cache now has {cache[0].shape[2]} positions\")\n",
    "    \n",
    "    print(\"\\n✓ Generation complete!\")\n",
    "    print(\"   Each new token only processes 1 position but attends to all previous.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the tests\n",
    "    test_kv_cache()\n",
    "    demonstrate_autoregressive_generation()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXERCISES TO TRY:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. Add causal masking to prevent attending to future tokens\")\n",
    "    print(\"2. Implement cache size limits (sliding window)\")\n",
    "    print(\"3. Add cross-attention with separate KV cache\")\n",
    "    print(\"4. Visualize attention weights with/without cache\")\n",
    "    print(\"5. Profile memory usage with longer sequences\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ea4a6-252d-4cec-9fd9-bf453ef786f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
